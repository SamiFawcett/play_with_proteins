{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"Attention is all you need\"\n",
    "\n",
    "What is a Transformer?\n",
    "\n",
    "Model sequence information that relys on the use of attention mechanisms to compute representations for the input and output.\n",
    "unlike RNN's, transformers allow for parallelization\n",
    "\n",
    "\n",
    "\n",
    "Enconder & Decoder stacks\n",
    "\n",
    "\n",
    "Encoder layer (Nx, where N = 6):\n",
    " - d_model = 512\n",
    " - multihead attention sublayer\n",
    " - ffn sublayer\n",
    " \n",
    " output of each sublayer looks like: LayerNorm(x + SubLayer(x)), where SubLayer is the function that is implemented byt the sublayer it self.\n",
    "\n",
    "\n",
    "Decoder Layer (N = 6)\n",
    " - In addition to the sublayers of the encoder, the decoder has a third layer. with a modification to the self attention mechanism to prevent positions from attending to subsequent positions.\n",
    " - The output embeddings are also offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention\n",
    " - can be described as a mapping of query and a set of key-value pairs to outputs\n",
    "\n",
    "Scaled Dot-Product Attention\n",
    "Multihead Attention\n",
    "\n",
    "\n",
    "\n",
    "most common types of attention:\n",
    "additive attention\n",
    "multiplicative attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position wise feed forward networks\n",
    "\n",
    "FFN(x) = max(0, xW1 + b1)W2 + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model doesn't inherently have knowledge about positional information of the sequence, we have to inject positional encodings to the input embeddings at the bottoms of the encoder and decoders stacks\n",
    "\n",
    "\n",
    "different types of positional encodings:\n",
    "PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos/10000^(2i/dmodel))\n",
    "\n",
    "Can also use learned embeddings instead, and found that the two versions produced nearly identical results. The chose to use the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than ones found in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three desiderata:\n",
    "\n",
    "One is the total computational complexity per layer. ANother is the amount of computational that can be parallelized,as mesured by the minimum number of sequential operatiosn required.\n",
    "\n",
    "\n",
    "The third is the path length between long range dependencies in the network."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
